{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 4,
    "colab": {
      "name": "Pytorch-Intro.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit ('base': conda)"
    },
    "interpreter": {
      "hash": "682cbd65bec36bb5780b3cd03d8f98f09d815121554481962b3929c5a83bba13"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/AvishekRoy16/DeepLearning/blob/master/6-Pytorch/Pytorch-Intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outline\n",
        "* PyTorch\n",
        "* What are tensors\n",
        "* Initialising, slicing, reshaping tensors\n",
        "* Numpy and PyTorch interfacing\n",
        "* GPU support for PyTorch + Enabling GPUs on Google Colab\n",
        "* Speed comparisons, Numpy -- PyTorch -- PyTorch on GPU\n",
        "* Autodiff concepts and application\n",
        "* Writing a basic learning loop using autograd\n",
        "* Exercises"
      ],
      "metadata": {
        "id": "HtySi30GgbrI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "outputs": [],
      "metadata": {
        "id": "iYnydU5PgbrL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensor is a kind of datastructure just like vector and matrics (list and dataframed/2D Lists).  \n",
        "They have a higher order and also many tensors have relation between them."
      ],
      "metadata": {
        "id": "ni7VI7pxgbrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialise Tensors"
      ],
      "metadata": {
        "id": "2oY_1ymxgbrM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "# Makes the tensors of the specified dimentions and fill them with ones\n",
        "x = torch.ones(3,2)\n",
        "print(x)\n",
        "\n",
        "# Makes the tensors of the specified dimentions and fill them with zeros\n",
        "x = torch.zeros(3, 2)\n",
        "print(x)\n",
        "\n",
        "# Makes the tensors of the specified dimentions and fill them with random numbers\n",
        "x = torch.rand(3, 2)\n",
        "print(x)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]])\n",
            "tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.]])\n",
            "tensor([[0.2029, 0.5901],\n",
            "        [0.5739, 0.3683],\n",
            "        [0.0375, 0.4813]])\n"
          ]
        }
      ],
      "metadata": {
        "id": "YoXUuoSagbrN",
        "outputId": "435ffeb3-d7d9-4179-e4a0-dabc7b853883",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "# Will create space for the dimentions spesified but will not initialise value in it\n",
        "x = torch.empty(3, 2)\n",
        "print(x)\n",
        "\n",
        "# if we want to give something the same shape as another tensor we can do that\n",
        "y = torch.zeros_like(x)\n",
        "print(y)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.5691e-07, 3.0887e-41],\n",
            "        [1.4487e-07, 3.0887e-41],\n",
            "        [8.9683e-44, 0.0000e+00]])\n",
            "tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.]])\n"
          ]
        }
      ],
      "metadata": {
        "id": "TPKx5FqJgbrO",
        "outputId": "501cc94f-6691-48c9-96d3-d7c40b427eb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "# Create a linearspace start, end, steps - start and end are included\n",
        "x = torch.linspace(0, 1, steps=5)\n",
        "print(x)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])\n"
          ]
        }
      ],
      "metadata": {
        "id": "Z7mi4o1qgbrO",
        "outputId": "56758f09-8b02-4d1c-f687-32fe416d265a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "# Manually defining the tensors\n",
        "x = torch.tensor([[1, 2], \n",
        "                 [3, 4], \n",
        "                 [5, 6]])\n",
        "print(x)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2],\n",
            "        [3, 4],\n",
            "        [5, 6]])\n"
          ]
        }
      ],
      "metadata": {
        "id": "kr_6XnjWgbrP",
        "outputId": "256d04a9-b411-4b11-9cc4-7748a50f3a64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Slicing tensors"
      ],
      "metadata": {
        "id": "1otPOhIFgbrP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "# Dimentions of the tensors are fiven insde a list\n",
        "print(x.size())\n",
        "\n",
        "# slicing[rows: column]\n",
        "# Take all rows and print the column of id 1\n",
        "print(x[:, 1]) \n",
        "# Take the 0th roaw and print all the column in that\n",
        "print(x[0, :])\n",
        "\n",
        "# All the rules for slicing in list apply to tensors as well"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 2])\n",
            "tensor([2, 4, 6])\n",
            "tensor([1, 2])\n"
          ]
        }
      ],
      "metadata": {
        "id": "YsArFExkgbrP",
        "outputId": "f23f28d1-cc5d-46ae-8be7-6333b573d5f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "# We are accessing a particular element from the x rows we are accessing the element in the \n",
        "# first row and first column, The data type of the element still remains tensor\n",
        "y = x[1, 1]\n",
        "print(y)\n",
        "# To change the data type of the element while accessing it from tensor to it's actual datatype.\n",
        "print(y.item())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4)\n",
            "4\n"
          ]
        }
      ],
      "metadata": {
        "id": "nfESHD8zgbrQ",
        "outputId": "5c848105-cefc-4065-bb4c-dcafcac9f8f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reshaping tensors"
      ],
      "metadata": {
        "id": "nThWGrldgbrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimentions play a very important role in machine learning and we have to keep track of what we are multiplying with what when we are trying to do matrix multiplications and other operation that erquire the dimentions of the tensors to be correct"
      ],
      "metadata": {
        "id": "TXQqniG4gbrQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "# To view the tensor in another dimentions we can use views - views(row, column)\n",
        "print(x)\n",
        "y = x.view(2, 3)\n",
        "print(y)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2],\n",
            "        [3, 4],\n",
            "        [5, 6]])\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n"
          ]
        }
      ],
      "metadata": {
        "id": "AwE1UVZ6gbrR",
        "outputId": "4e58813c-03c7-4f60-ed23-f56876779577",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "# We can reshape it when we know pnly one of the dimentions and \n",
        "# it will pick and appropriate number to put in the second dimention, to do that we have to fill -1 in\n",
        "# the dimention we do not know the number\n",
        "y = x.view(6,-1) \n",
        "print(y)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1],\n",
            "        [2],\n",
            "        [3],\n",
            "        [4],\n",
            "        [5],\n",
            "        [6]])\n"
          ]
        }
      ],
      "metadata": {
        "id": "B5j7_-xSgbrR",
        "outputId": "5b925eda-ac18-4a04-c77c-f406a49ff875",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Tensor Operations"
      ],
      "metadata": {
        "id": "aZMmypxUgbrR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "# Simple operations in Tesors\n",
        "x = torch.ones([3, 2])\n",
        "y = torch.ones([3, 2])\n",
        "z = x + y\n",
        "print(z)\n",
        "z = x - y\n",
        "print(z)\n",
        "z = x * y\n",
        "print(z)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 2.],\n",
            "        [2., 2.],\n",
            "        [2., 2.]])\n",
            "tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.]])\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]])\n"
          ]
        }
      ],
      "metadata": {
        "id": "lScJTCgOgbrS",
        "outputId": "ae4b9f10-9b87-41cf-858e-0fa40e739f69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "# z is being updated by adding x to y. Here y reamins the same and is not updated\n",
        "z = y.add(x)\n",
        "print(z)\n",
        "print(y)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 2.],\n",
            "        [2., 2.],\n",
            "        [2., 2.]])\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]])\n"
          ]
        }
      ],
      "metadata": {
        "id": "CvUERtHhgbrS",
        "outputId": "3cf980d2-d678-434c-f71d-d6a2ca1778a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "# Addition in place\n",
        "# We are taking y and then adding x to it and updating y in the process\n",
        "z = y.add_(x)\n",
        "print(z)\n",
        "print(y)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 2.],\n",
            "        [2., 2.],\n",
            "        [2., 2.]])\n",
            "tensor([[2., 2.],\n",
            "        [2., 2.],\n",
            "        [2., 2.]])\n"
          ]
        }
      ],
      "metadata": {
        "id": "TfBxDzFngbrS",
        "outputId": "61d9e148-8c95-4e84-f578-a4bcaed8015c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Numpy <> PyTorch"
      ],
      "metadata": {
        "id": "aeSaKPA8gbrT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "# Interfacing Numpy and Pytorch\n",
        "\n",
        "# Converted tensor into numpy\n",
        "x_np = x.numpy()\n",
        "print(type(x), type(x_np))\n",
        "print(x_np)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'> <class 'numpy.ndarray'>\n",
            "[[1. 1.]\n",
            " [1. 1.]\n",
            " [1. 1.]]\n"
          ]
        }
      ],
      "metadata": {
        "id": "b30q1y8vgbrT",
        "outputId": "e238f250-4a0d-4e3d-def6-0110b17e82fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "# Converting a numpy array into a tenors\n",
        "a = np.random.randn(5)\n",
        "print(a)\n",
        "a_pt = torch.from_numpy(a)\n",
        "print(type(a), type(a_pt))\n",
        "print(a_pt)\n",
        "# This is less of copying and more a bridge between the two as if we make changes into numpy,\n",
        "# it will be reflected in tensor"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.62234009  0.86093278 -1.85977867 -0.77231315 -0.83015987]\n",
            "<class 'numpy.ndarray'> <class 'torch.Tensor'>\n",
            "tensor([ 0.6223,  0.8609, -1.8598, -0.7723, -0.8302], dtype=torch.float64)\n"
          ]
        }
      ],
      "metadata": {
        "id": "TZo-GqLFgbrT",
        "outputId": "d5061d50-c565-40a8-a995-1d285cffa22e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "np.add(a, 1, out=a)\n",
        "print(a)\n",
        "print(a_pt) "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.62234009  1.86093278 -0.85977867  0.22768685  0.16984013]\n",
            "tensor([ 1.6223,  1.8609, -0.8598,  0.2277,  0.1698], dtype=torch.float64)\n"
          ]
        }
      ],
      "metadata": {
        "id": "89pH6iatgbrU",
        "outputId": "e1ba7f07-adae-4fbc-be90-7a050151617b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "%%time\n",
        "# Checking the time taken to loop and add random numbers using numpy arrays\n",
        "for i in range(100):\n",
        "  a = np.random.randn(100,100) # (100,100) is the matrix size\n",
        "  b = np.random.randn(100,100)\n",
        "  c = np.matmul(a, b)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 369 ms, sys: 6.8 ms, total: 375 ms\n",
            "Wall time: 112 ms\n"
          ]
        }
      ],
      "metadata": {
        "id": "ARMKGnbbgbrU",
        "outputId": "d2fbf641-7df1-4790-c333-7f1e45254fa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "%%time\n",
        "# Checking the time taken to loop and add random numbers using tensors\n",
        "for i in range(100):\n",
        "  a = torch.randn([100, 100])\n",
        "  b = torch.randn([100, 100])\n",
        "  c = torch.matmul(a, b)\n",
        "\n",
        "# Note we are still not using the GPU."
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 58.4 ms, sys: 0 ns, total: 58.4 ms\n",
            "Wall time: 26.6 ms\n"
          ]
        }
      ],
      "metadata": {
        "id": "VqUpIHOogbrU",
        "outputId": "4ff209a4-74df-40b9-b785-6fe5bba61647",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "%%time\n",
        "for i in range(10):\n",
        "  a = np.random.randn(10000,10000)\n",
        "  b = np.random.randn(10000,10000)\n",
        "  c = a + b"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 55.7 s, sys: 14.1 s, total: 1min 9s\n",
            "Wall time: 1min 11s\n"
          ]
        }
      ],
      "metadata": {
        "id": "seHZ5m6DgbrU",
        "outputId": "0b4ecbb7-d71e-484f-962a-3eb68920c659",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "source": [
        "%%time\n",
        "for i in range(10):\n",
        "  a = torch.randn([10000, 10000])\n",
        "  b = torch.randn([10000, 10000])\n",
        "  c = a + b"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 15.8 s, sys: 4.51 s, total: 20.3 s\n",
            "Wall time: 16.7 s\n"
          ]
        }
      ],
      "metadata": {
        "id": "GjFREOFhgbrV",
        "outputId": "008d9146-40a0-479b-8979-ba05984e5c8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CUDA support"
      ],
      "metadata": {
        "id": "d7M80mg7gbrV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "source": [
        "print(torch.cuda.device_count())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ],
      "metadata": {
        "id": "2C00MB94gbrV",
        "outputId": "b7d8896f-1874-4a2c-bf07-ffeb479b2292",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "source": [
        "print(torch.cuda.device(0))\n",
        "print(torch.cuda.get_device_name(0))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.cuda.device object at 0x7f1b5e605280>\n",
            "GeForce MX150\n"
          ]
        }
      ],
      "metadata": {
        "id": "1Uy6MkvTgbrW",
        "outputId": "7974da2a-2ba4-4b5f-926d-2c79b361705c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "source": [
        "cuda0 = torch.device('cuda:0')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "source": [
        "a = torch.ones(3, 2, device=cuda0)\n",
        "b = torch.ones(3, 2, device=cuda0)\n",
        "c = a + b\n",
        "print(c)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 2.],\n",
            "        [2., 2.],\n",
            "        [2., 2.]], device='cuda:0')\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "source": [
        "print(a)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]], device='cuda:0')\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "source": [
        "%%time\n",
        "# Time comparison between numpy, cpu and gpu performance for matrix addition\n",
        "for i in range(10):\n",
        "  a = np.random.randn(10000,10000)\n",
        "  b = np.random.randn(10000,10000)\n",
        "  np.add(b, a)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 5s, sys: 17.6 s, total: 1min 23s\n",
            "Wall time: 1min 24s\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "source": [
        "%%time\n",
        "for i in range(10):\n",
        "  a_cpu = torch.randn([10000, 10000])\n",
        "  b_cpu = torch.randn([10000, 10000])\n",
        "  b_cpu.add_(a_cpu)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 24.1 s, sys: 3.97 s, total: 28.1 s\n",
            "Wall time: 23.4 s\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "source": [
        "%%time\n",
        "for i in range(10):\n",
        "  a = torch.randn([10000, 10000], device=cuda0)\n",
        "  b = torch.randn([10000, 10000], device=cuda0)\n",
        "  b.add_(a)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 9.67 ms, sys: 59.4 ms, total: 69.1 ms\n",
            "Wall time: 80.9 ms\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "source": [
        "%%time\n",
        "# Time comparison between numpy, cpu and gpu performance for matrix multiplication\n",
        "for i in range(10):\n",
        "  a = np.random.randn(10000,10000)\n",
        "  b = np.random.randn(10000,10000)\n",
        "  np.matmul(b, a)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 13min 7s, sys: 19.1 s, total: 13min 26s\n",
            "Wall time: 4min 4s\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "source": [
        "%%time\n",
        "for i in range(10):\n",
        "  a_cpu = torch.randn([10000, 10000])\n",
        "  b_cpu = torch.randn([10000, 10000])\n",
        "  torch.matmul(a_cpu, b_cpu)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 6min 41s, sys: 4.5 s, total: 6min 45s\n",
            "Wall time: 1min 53s\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "source": [
        "%%time\n",
        "for i in range(10):\n",
        "  a = torch.randn([10000, 10000], device=cuda0)\n",
        "  b = torch.randn([10000, 10000], device=cuda0)\n",
        "  torch.matmul(a, b)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 543 ms, sys: 232 ms, total: 775 ms\n",
            "Wall time: 1.24 s\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autodiff"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "This feature lets us calculate the gradient automatically"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "source": [
        "# requires grad = True is spefied when the tensor is used in autodiff\n",
        "# so we might later differenciate therse wrt x\n",
        "x = torch.ones([3,2], requires_grad = True)\n",
        "print(x)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]], requires_grad=True)\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensor on a data-structure level is about storing these multidimentional matrices but further at a structural level it also relates different tensors with each other.  \r\n",
        "The ability to model different high dimentional matrices is what makes tensors what they are"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "source": [
        "# when we do this y automatically understands that y is a function of x which itself requires gradients\n",
        "y = x + 5\n",
        "print(y)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6., 6.],\n",
            "        [6., 6.],\n",
            "        [6., 6.]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "source": [
        "# so we are stackin another fuction on top of y\n",
        "z = y*y + 1\n",
        "print(z)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[37., 37.],\n",
            "        [37., 37.],\n",
            "        [37., 37.]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "source": [
        "# torch.sum simply adds all the numbers in the tensors\n",
        "t = torch.sum(z)\n",
        "print(t)\n",
        "# We can think of this as a forward pass we have been doing\n",
        "# The book-keeping is being kept bu pyTorch and it tells that the last operation being done on the tensor was sum operation"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(222., grad_fn=<SumBackward0>)\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "source": [
        "# At this point we are ready to do a backward pass\n",
        "t.backward()\n",
        "# nothing is shown in the output, pyTorch is internally doing some computations "
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "source": [
        "# x.grad is the derivative of t wrt x\n",
        "# We had taken the backward starting from t so that becomes the fucntion that we want to diffrenciate\n",
        "# ans we want do diffrenciate it against x upon which we are calling the grad\n",
        "print(x.grad)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[12., 12.],\n",
            "        [12., 12.],\n",
            "        [12., 12.]])\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logic for why the derivative of t wrt x was 12:  \r\n",
        "$t = \\sum_i z_i,    z_i = y_i^2 + 1,    y_i = x_i + 5$\r\n",
        "\r\n",
        "$\\frac{\\partial t}{\\partial x_i} = \\frac{\\partial z_i}{\\partial x_i} = \\frac{\\partial z_i}{\\partial y_i} \\frac{\\partial y_i}{\\partial x_i} = 2y_i \\times 1$\r\n",
        "\r\n",
        "\r\n",
        "At x = 1, y = 6, $\\frac{\\partial t}{\\partial x_i} = 12$"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "So here we can see that we are getting the partial derivative of t wrt all x's.  \r\n",
        "We can now write any cascading set on functions on a given set of inputs,  \r\n",
        "we can call trig functions, tanh, log, even things like standard deviation mean and so on, \r\n",
        "then compute derivative wrt inputs\r\n",
        "\r\n",
        "This is numerically being computed at the poin we have initialised our values. So in this particular example, x is initialised at 1's."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "source": [
        "# Another example, x and y values replacing z by r where r is taking the sigmoid of y\n",
        "x = torch.ones([3, 2], requires_grad=True)\n",
        "y = x + 5\n",
        "r = 1/(1 + torch.exp(-y))\n",
        "print(r)\n",
        "s = torch.sum(r)\n",
        "s.backward()\n",
        "print(x.grad)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9975, 0.9975],\n",
            "        [0.9975, 0.9975],\n",
            "        [0.9975, 0.9975]], grad_fn=<MulBackward0>)\n",
            "tensor([[0.0025, 0.0025],\n",
            "        [0.0025, 0.0025],\n",
            "        [0.0025, 0.0025]])\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We were earlier writing the forward pass and backward pass ourselves and were implementing our knowlege of the derivative of sigmoids, tanh etc. now we are letting pyTorch do it for us automatically. So it's quite a powerful thing in that sense!"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "source": [
        "# We can do the above diffrensiation in this manne too, just that here instad of taking r into s and summing it to get one value\n",
        "# we define 'a' with 1's and the same shape of x.\n",
        "# So basicallly we are avoiding calling the sum fuction\n",
        "x = torch.ones([3, 2], requires_grad=True)\n",
        "y = x + 5\n",
        "r = 1/(1 + torch.exp(-y))\n",
        "a = torch.ones([3, 2])\n",
        "r.backward(a)\n",
        "print(x.grad)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0025, 0.0025],\n",
            "        [0.0025, 0.0025],\n",
            "        [0.0025, 0.0025]])\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "r.backward is computing the derivative of r wrt x, but it multiplies point wise with derivative the value of 'a' which we  have taken as an argument in r.backward   \r\n",
        "so we are doing $\\frac{\\partial{s}}{\\partial{r}}$ and multiplying poin wise with 'a'.  \r\n",
        "\r\n",
        "This feature is there so that we are able to cascade our chain rule through multiple fucntions  "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\frac{\\partial{s}}{\\partial{x}} = \\frac{\\partial{s}}{\\partial{r}} \\cdot \\frac{\\partial{r}}{\\partial{x}}$\r\n",
        "\r\n",
        "For the above code $a$ represents $\\frac{\\partial{s}}{\\partial{r}}$ and then $x.grad$ gives directly $\\frac{\\partial{s}}{\\partial{x}}$\r\n",
        "\r\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "id we want to calculate $\\frac{\\partial{s}}{\\partial{x}}$ then it is given by chain rule $\\frac{\\partial{s}}{\\partial{r}} \\cdot \\frac{\\partial{r}}{\\partial{x}}$  \r\n",
        "So we want to move from s to, so we first move from s to r and then form r to x.  \r\n",
        "r to x is given by r.backward, but if we have already computed s to r and stored that in a then point wise multiplying a with $\\frac{\\partial{r}}{\\partial{x}}$ will directly give us $\\frac{\\partial{s}}{\\partial{x}}$  \r\n",
        "In this case we dont have a s we want to concider so, When a is a submission we will use torch.ones"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autodiff example that looks like what we have been doing"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Forward Pass"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "source": [
        "# Think of these as a training dataset, with 20 items \n",
        "# for each of them we are computing y = 3*x - w we can think of this as the gorund truth model - real output\n",
        "# Input data\n",
        "x = torch.randn([20, 1], requires_grad=True)\n",
        "# Model\n",
        "y = 3*x - 2"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "source": [
        "# We don't know the correct values of w and b so we are setting them up to be we 1 each, \n",
        "# the real values we know 3 and -2 from y = 3x - 2\n",
        "w = torch.tensor([1.], requires_grad=True)\n",
        "b = torch.tensor([1.], requires_grad=True)\n",
        "\n",
        "# This is the predicted value\n",
        "y_hat = w*x + b\n",
        "\n",
        "# We are using the usual MSE - Mean Squared Error, not taking the mean here\n",
        "loss = torch.sum((y_hat - y)**2)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "source": [
        "print(loss)\n",
        "# The loss is a positve number, shows that the model is not equal, \n",
        "# i.e is the estimated values of w and b are not the same as the ground truth model"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(290.7623, grad_fn=<SumBackward0>)\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Backprpogatoin"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "source": [
        "loss.backward()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "source": [
        "# Derivative of loss wrt w and b respectively\n",
        "print(w.grad, b.grad)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-107.0340]) tensor([122.4855])\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations: We know that the model that we are using is a simple linear model. So the correct value of w should be 3, and we estimated the value to be 1. So the w whould increase, which is clear as the derivative of loss wrt to w is negetive.\r\n",
        "This means if we move towards increasing w the loss will decrease.  \r\n",
        "\r\n",
        "On the other hand for b we have chosen to 1 as the value and the actual value is -2, so we would like to decrease it. So if we take a derivative of loss wrt to b, as we increase b the loss should increase\r\n",
        "\r\n",
        "So if the gradient is a positive number then, then the selected initialsation value should decrease and vice versa."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Do it in a loop"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "source": [
        "# Taking Learning rate as a constant\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Initiating w and b to 1 in and enabling the autodiff functionality\n",
        "w = torch.tensor([1.], requires_grad=True)\n",
        "b = torch.tensor([1.], requires_grad=True)\n",
        "# Printing the initial values of w and b\n",
        "print(w.item(), b.item())\n",
        "\n",
        "# FORWARD PROPAGATION - also known as computation graph(where different varibles are interactinga and computing new variables)\n",
        "# Now we are going through the learning loop so we can think of 10 here as the number of epochs\n",
        "for i in range(10):\n",
        "  # Input values\n",
        "  x = torch.randn([20, 1])\n",
        "  # Ground truth model\n",
        "  y = 3*x - 2\n",
        "  \n",
        "  # Predicted Value\n",
        "  y_hat = w*x + b\n",
        "  # Loss Function\n",
        "  loss = torch.sum((y_hat - y)**2)\n",
        "  \n",
        "  \n",
        "  # BACKWARD PROPAGATION\n",
        "  loss.backward()\n",
        "  \n",
        "  # Standard Gradient Decent Algo(Update Rule)\n",
        "  # we have started backward propagation but to tell pyTorch that we have stated backward \n",
        "  # propagation so that it does not continue it's book keeping and build relations b/w the tensors \n",
        "  # so we use with torch.no_grad\n",
        "  # so the w and b are jsut being thought as variable updates\n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate * w.grad\n",
        "    b -= learning_rate * b.grad\n",
        "    \n",
        "    # We also need to set the gradients to zero so that they are completely fresh\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_() \n",
        "\n",
        "  print(w.item(), b.item())\n",
        "  # as we know the write values of w and b are 3 and -2 and the output shows that we are coming close to the actual values"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0 1.0\n",
            "1.8591370582580566 -0.16979622840881348\n",
            "2.4375343322753906 -0.7341733574867249\n",
            "2.5145201683044434 -1.2211039066314697\n",
            "2.670606851577759 -1.5503445863723755\n",
            "2.733276844024658 -1.711594581604004\n",
            "2.800959587097168 -1.8045190572738647\n",
            "2.809910774230957 -1.8447062969207764\n",
            "2.880446434020996 -1.884988784790039\n",
            "2.911482334136963 -1.9189097881317139\n",
            "2.927065849304199 -1.9481284618377686\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Do it for a large problem"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "source": [
        "%%time\n",
        "# Using pyTorch but not GPU\n",
        "learning_rate = 0.001\n",
        "N = 10000000\n",
        "epochs = 200\n",
        "\n",
        "# Here w is a vector of N values - so we get the same benifit of vectorisation we had earlier seen in numpy\n",
        "# w is randomised b/w 0 and 1\n",
        "w = torch.rand([N], requires_grad=True)\n",
        "b = torch.ones([1], requires_grad=True)\n",
        "\n",
        "# print(torch.mean(w).item(), b.item())\n",
        "\n",
        "for i in range(epochs):\n",
        "  \n",
        "  # x contains random numbers but there are N features\n",
        "  x = torch.randn([N])\n",
        "  y = torch.dot(3*torch.ones([N]), x) - 2\n",
        "  \n",
        "  # The model has a dot product b/w w and x\n",
        "  y_hat = torch.dot(w, x) + b\n",
        "  loss = torch.sum((y_hat - y)**2)\n",
        "  \n",
        "  loss.backward()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate * w.grad\n",
        "    b -= learning_rate * b.grad\n",
        "    \n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "\n",
        "  # print(torch.mean(w).item(), b.item())\n",
        "  \n",
        "  \n",
        "  # The correct value for w is 3 and for b it us -2 form the equation -  y = torch.dot(3*torch.ones([N]), x) - 2\n",
        "  # We can see from the output that there are ossilations but the model did come close to original value of w which is 3\n",
        "  # and b needs more number of epochs to being learned by this model"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 22s, sys: 22.8 s, total: 1min 45s\n",
            "Wall time: 38 s\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "source": [
        "%%time\n",
        "# Using pyTorch & GPU\n",
        "learning_rate = 0.001\n",
        "N = 10000000\n",
        "epochs = 200\n",
        "\n",
        "# Every tensor that we create should be in the gpu\n",
        "\n",
        "# instanciating the waribles with the GPU\n",
        "w = torch.rand([N], requires_grad=True, device=cuda0)\n",
        "b = torch.ones([1], requires_grad=True, device=cuda0)\n",
        "\n",
        "# print(torch.mean(w).item(), b.item())\n",
        "\n",
        "for i in range(epochs):\n",
        "  \n",
        "  x = torch.randn([N], device=cuda0)\n",
        "  # here we cans see that the ones also has to be declared in the cuda device\n",
        "  y = torch.dot(3*torch.ones([N], device=cuda0), x) - 2\n",
        "  \n",
        "  y_hat = torch.dot(w, x) + b\n",
        "  loss = torch.sum((y_hat - y)**2)\n",
        "  \n",
        "  loss.backward()\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate * w.grad\n",
        "    b -= learning_rate * b.grad\n",
        "    \n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "\n",
        "  #print(torch.mean(w).item(), b.item())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.02 s, sys: 908 ms, total: 2.93 s\n",
            "Wall time: 2.95 s\n"
          ]
        }
      ],
      "metadata": {}
    }
  ]
}