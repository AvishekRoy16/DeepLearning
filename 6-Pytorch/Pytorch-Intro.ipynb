{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 4,
    "colab": {
      "name": "Pytorch-Intro.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/AvishekRoy16/DeepLearning/blob/master/6-Pytorch/Pytorch-Intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outline\n",
        "* PyTorch\n",
        "* What are tensors\n",
        "* Initialising, slicing, reshaping tensors\n",
        "* Numpy and PyTorch interfacing\n",
        "* GPU support for PyTorch + Enabling GPUs on Google Colab\n",
        "* Speed comparisons, Numpy -- PyTorch -- PyTorch on GPU\n",
        "* Autodiff concepts and application\n",
        "* Writing a basic learning loop using autograd\n",
        "* Exercises"
      ],
      "metadata": {
        "id": "HtySi30GgbrI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "import torch\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "outputs": [],
      "metadata": {
        "id": "iYnydU5PgbrL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensor is a kind of datastructure just like vector and matrics (list and dataframed/2D Lists).  \n",
        "They have a higher order and also many tensors have relation between them."
      ],
      "metadata": {
        "id": "ni7VI7pxgbrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialise Tensors"
      ],
      "metadata": {
        "id": "2oY_1ymxgbrM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "source": [
        "# Makes the tensors of the specified dimentions and fill them with ones\r\n",
        "x = torch.ones(3,2)\r\n",
        "print(x)\r\n",
        "\r\n",
        "# Makes the tensors of the specified dimentions and fill them with zeros\r\n",
        "x = torch.zeros(3, 2)\r\n",
        "print(x)\r\n",
        "\r\n",
        "# Makes the tensors of the specified dimentions and fill them with random numbers\r\n",
        "x = torch.rand(3, 2)\r\n",
        "print(x)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]])\n",
            "tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.]])\n",
            "tensor([[0.9788, 0.2605],\n",
            "        [0.8890, 0.1032],\n",
            "        [0.2772, 0.7433]])\n"
          ]
        }
      ],
      "metadata": {
        "id": "YoXUuoSagbrN",
        "outputId": "435ffeb3-d7d9-4179-e4a0-dabc7b853883",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "source": [
        "# Will create space for the dimentions spesified but will not initialise value in it\r\n",
        "x = torch.empty(3, 2)\r\n",
        "print(x)\r\n",
        "\r\n",
        "# if we want to give something the same shape as another tensor we can do that\r\n",
        "y = torch.zeros_like(x)\r\n",
        "print(y)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6.5650e+28, 1.7788e+25],\n",
            "        [1.3425e+13, 1.1168e+33],\n",
            "        [2.5348e-09, 1.0765e+21]])\n",
            "tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.]])\n"
          ]
        }
      ],
      "metadata": {
        "id": "TPKx5FqJgbrO",
        "outputId": "501cc94f-6691-48c9-96d3-d7c40b427eb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "source": [
        "# Create a linearspace start, end, steps - start and end are included\r\n",
        "x = torch.linspace(0, 1, steps=5)\r\n",
        "print(x)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])\n"
          ]
        }
      ],
      "metadata": {
        "id": "Z7mi4o1qgbrO",
        "outputId": "56758f09-8b02-4d1c-f687-32fe416d265a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "source": [
        "# Manually defining the tensors\r\n",
        "x = torch.tensor([[1, 2], \r\n",
        "                 [3, 4], \r\n",
        "                 [5, 6]])\r\n",
        "print(x)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2],\n",
            "        [3, 4],\n",
            "        [5, 6]])\n"
          ]
        }
      ],
      "metadata": {
        "id": "kr_6XnjWgbrP",
        "outputId": "256d04a9-b411-4b11-9cc4-7748a50f3a64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Slicing tensors"
      ],
      "metadata": {
        "id": "1otPOhIFgbrP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "source": [
        "# Dimentions of the tensors are fiven insde a list\r\n",
        "print(x.size())\r\n",
        "\r\n",
        "# slicing[rows: column]\r\n",
        "# Take all rows and print the column of id 1\r\n",
        "print(x[:, 1]) \r\n",
        "# Take the 0th roaw and print all the column in that\r\n",
        "print(x[0, :])\r\n",
        "\r\n",
        "# All the rules for slicing in list apply to tensors as well"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 2])\n",
            "tensor([2, 4, 6])\n",
            "tensor([1, 2])\n"
          ]
        }
      ],
      "metadata": {
        "id": "YsArFExkgbrP",
        "outputId": "f23f28d1-cc5d-46ae-8be7-6333b573d5f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "source": [
        "# We are accessing a particular element from the x rows we are accessing the element in the \r\n",
        "# first row and first column, The data type of the element still remains tensor\r\n",
        "y = x[1, 1]\r\n",
        "print(y)\r\n",
        "# To change the data type of the element while accessing it from tensor to it's actual datatype.\r\n",
        "print(y.item())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4)\n",
            "4\n"
          ]
        }
      ],
      "metadata": {
        "id": "nfESHD8zgbrQ",
        "outputId": "5c848105-cefc-4065-bb4c-dcafcac9f8f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reshaping tensors"
      ],
      "metadata": {
        "id": "nThWGrldgbrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimentions play a very important role in machine learning and we have to keep track of what we are multiplying with what when we are trying to do matrix multiplications and other operation that erquire the dimentions of the tensors to be correct"
      ],
      "metadata": {
        "id": "TXQqniG4gbrQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "source": [
        "# To view the tensor in another dimentions we can use views - views(row, column)\r\n",
        "print(x)\r\n",
        "y = x.view(2, 3)\r\n",
        "print(y)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2],\n",
            "        [3, 4],\n",
            "        [5, 6]])\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n"
          ]
        }
      ],
      "metadata": {
        "id": "AwE1UVZ6gbrR",
        "outputId": "4e58813c-03c7-4f60-ed23-f56876779577",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "source": [
        "# We can reshape it when we know pnly one of the dimentions and \r\n",
        "# it will pick and appropriate number to put in the second dimention, to do that we have to fill -1 in\r\n",
        "# the dimention we do not know the number\r\n",
        "y = x.view(6,-1) \r\n",
        "print(y)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1],\n",
            "        [2],\n",
            "        [3],\n",
            "        [4],\n",
            "        [5],\n",
            "        [6]])\n"
          ]
        }
      ],
      "metadata": {
        "id": "B5j7_-xSgbrR",
        "outputId": "5b925eda-ac18-4a04-c77c-f406a49ff875",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Tensor Operations"
      ],
      "metadata": {
        "id": "aZMmypxUgbrR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "source": [
        "# Simple operations in Tesors\r\n",
        "x = torch.ones([3, 2])\r\n",
        "y = torch.ones([3, 2])\r\n",
        "z = x + y\r\n",
        "print(z)\r\n",
        "z = x - y\r\n",
        "print(z)\r\n",
        "z = x * y\r\n",
        "print(z)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 2.],\n",
            "        [2., 2.],\n",
            "        [2., 2.]])\n",
            "tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.]])\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]])\n"
          ]
        }
      ],
      "metadata": {
        "id": "lScJTCgOgbrS",
        "outputId": "ae4b9f10-9b87-41cf-858e-0fa40e739f69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "source": [
        "# z is being updated by adding x to y. Here y reamins the same and is not updated\r\n",
        "z = y.add(x)\r\n",
        "print(z)\r\n",
        "print(y)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 2.],\n",
            "        [2., 2.],\n",
            "        [2., 2.]])\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]])\n"
          ]
        }
      ],
      "metadata": {
        "id": "CvUERtHhgbrS",
        "outputId": "3cf980d2-d678-434c-f71d-d6a2ca1778a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "source": [
        "# Addition in place\r\n",
        "# We are taking y and then adding x to it and updating y in the process\r\n",
        "z = y.add_(x)\r\n",
        "print(z)\r\n",
        "print(y)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 2.],\n",
            "        [2., 2.],\n",
            "        [2., 2.]])\n",
            "tensor([[2., 2.],\n",
            "        [2., 2.],\n",
            "        [2., 2.]])\n"
          ]
        }
      ],
      "metadata": {
        "id": "TfBxDzFngbrS",
        "outputId": "61d9e148-8c95-4e84-f578-a4bcaed8015c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Numpy <> PyTorch"
      ],
      "metadata": {
        "id": "aeSaKPA8gbrT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "source": [
        "# Interfacing Numpy and Pytorch\r\n",
        "\r\n",
        "# Converted tensor into numpy\r\n",
        "x_np = x.numpy()\r\n",
        "print(type(x), type(x_np))\r\n",
        "print(x_np)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'> <class 'numpy.ndarray'>\n",
            "[[1. 1.]\n",
            " [1. 1.]\n",
            " [1. 1.]]\n"
          ]
        }
      ],
      "metadata": {
        "id": "b30q1y8vgbrT",
        "outputId": "e238f250-4a0d-4e3d-def6-0110b17e82fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "source": [
        "# Converting a numpy array into a tenors\r\n",
        "a = np.random.randn(5)\r\n",
        "print(a)\r\n",
        "a_pt = torch.from_numpy(a)\r\n",
        "print(type(a), type(a_pt))\r\n",
        "print(a_pt)\r\n",
        "# This is less of copying and more a bridge between the two as if we make changes into numpy,\r\n",
        "# it will be reflected in tensor"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.40097206 -0.37719441 -0.30400602  0.85176317 -2.5799248 ]\n",
            "<class 'numpy.ndarray'> <class 'torch.Tensor'>\n",
            "tensor([ 0.4010, -0.3772, -0.3040,  0.8518, -2.5799], dtype=torch.float64)\n"
          ]
        }
      ],
      "metadata": {
        "id": "TZo-GqLFgbrT",
        "outputId": "d5061d50-c565-40a8-a995-1d285cffa22e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "source": [
        "np.add(a, 1, out=a)\r\n",
        "print(a)\r\n",
        "print(a_pt) "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.40097206  0.62280559  0.69599398  1.85176317 -1.5799248 ]\n",
            "tensor([ 1.4010,  0.6228,  0.6960,  1.8518, -1.5799], dtype=torch.float64)\n"
          ]
        }
      ],
      "metadata": {
        "id": "89pH6iatgbrU",
        "outputId": "e1ba7f07-adae-4fbc-be90-7a050151617b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "source": [
        "%%time\r\n",
        "# Checking the time taken to loop and add random numbers using numpy arrays\r\n",
        "for i in range(100):\r\n",
        "  a = np.random.randn(100,100) # (100,100) is the matrix size\r\n",
        "  b = np.random.randn(100,100)\r\n",
        "  c = np.matmul(a, b)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wall time: 429 ms\n"
          ]
        }
      ],
      "metadata": {
        "id": "ARMKGnbbgbrU",
        "outputId": "d2fbf641-7df1-4790-c333-7f1e45254fa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "source": [
        "%%time\r\n",
        "# Checking the time taken to loop and add random numbers using tensors\r\n",
        "for i in range(100):\r\n",
        "  a = torch.randn([100, 100])\r\n",
        "  b = torch.randn([100, 100])\r\n",
        "  c = torch.matmul(a, b)\r\n",
        "\r\n",
        "# Note we are still not using the GPU."
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wall time: 501 ms\n"
          ]
        }
      ],
      "metadata": {
        "id": "VqUpIHOogbrU",
        "outputId": "4ff209a4-74df-40b9-b785-6fe5bba61647",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "source": [
        "%%time\r\n",
        "for i in range(10):\r\n",
        "  a = np.random.randn(10000,10000)\r\n",
        "  b = np.random.randn(10000,10000)\r\n",
        "  c = a + b"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wall time: 1min 19s\n"
          ]
        }
      ],
      "metadata": {
        "id": "seHZ5m6DgbrU",
        "outputId": "0b4ecbb7-d71e-484f-962a-3eb68920c659",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "source": [
        "%%time\r\n",
        "for i in range(10):\r\n",
        "  a = torch.randn([10000, 10000])\r\n",
        "  b = torch.randn([10000, 10000])\r\n",
        "  c = a + b"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wall time: 23 s\n"
          ]
        }
      ],
      "metadata": {
        "id": "GjFREOFhgbrV",
        "outputId": "008d9146-40a0-479b-8979-ba05984e5c8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CUDA support"
      ],
      "metadata": {
        "id": "d7M80mg7gbrV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "source": [
        "print(torch.cuda.device_count())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ],
      "metadata": {
        "id": "2C00MB94gbrV",
        "outputId": "b7d8896f-1874-4a2c-bf07-ffeb479b2292",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "source": [
        "print(torch.cuda.device(0))\r\n",
        "print(torch.cuda.get_device_name(0))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.cuda.device object at 0x000002377BD370A0>\n",
            "NVIDIA GeForce MX150\n"
          ]
        }
      ],
      "metadata": {
        "id": "1Uy6MkvTgbrW",
        "outputId": "7974da2a-2ba4-4b5f-926d-2c79b361705c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "source": [
        "cuda0 = torch.device('cuda:0')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "source": [
        "a = torch.ones(3, 2, device=cuda0)\r\n",
        "b = torch.ones(3, 2, device=cuda0)\r\n",
        "c = a + b\r\n",
        "print(c)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 2.],\n",
            "        [2., 2.],\n",
            "        [2., 2.]], device='cuda:0')\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "source": [
        "print(a)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]], device='cuda:0')\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "source": [
        "%%time\r\n",
        "# Time comparison between numpy, cpu and gpu performance for matrix addition\r\n",
        "for i in range(10):\r\n",
        "  a = np.random.randn(10000,10000)\r\n",
        "  b = np.random.randn(10000,10000)\r\n",
        "  np.add(b, a)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wall time: 1min 17s\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "source": [
        "%%time\r\n",
        "for i in range(10):\r\n",
        "  a_cpu = torch.randn([10000, 10000])\r\n",
        "  b_cpu = torch.randn([10000, 10000])\r\n",
        "  b_cpu.add_(a_cpu)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wall time: 21.8 s\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "source": [
        "%%time\r\n",
        "for i in range(10):\r\n",
        "  a = torch.randn([10000, 10000], device=cuda0)\r\n",
        "  b = torch.randn([10000, 10000], device=cuda0)\r\n",
        "  b.add_(a)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wall time: 2.66 s\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "source": [
        "%%time\r\n",
        "# Time comparison between numpy, cpu and gpu performance for matrix multiplication\r\n",
        "for i in range(10):\r\n",
        "  a = np.random.randn(10000,10000)\r\n",
        "  b = np.random.randn(10000,10000)\r\n",
        "  np.matmul(b, a)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wall time: 7min 14s\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "source": [
        "%%time\r\n",
        "for i in range(10):\r\n",
        "  a_cpu = torch.randn([10000, 10000])\r\n",
        "  b_cpu = torch.randn([10000, 10000])\r\n",
        "  torch.matmul(a_cpu, b_cpu)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wall time: 3min 11s\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "source": [
        "%%time\r\n",
        "for i in range(10):\r\n",
        "  a = torch.randn([10000, 10000], device=cuda0)\r\n",
        "  b = torch.randn([10000, 10000], device=cuda0)\r\n",
        "  torch.matmul(a, b)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wall time: 14.7 s\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autodiff"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "This feature lets us calculate the gradient automatically"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "# requires grad = True is spefied when the tensor is used in autodiff\r\n",
        "# so we might later differenciate therse wrt x\r\n",
        "x = torch.ones([3,2], requires_grad = True)\r\n",
        "print(x)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]], requires_grad=True)\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensor on a data-structure level is about storing these multidimentional matrices but further at a structural level it also relates different tensors with each other.  \r\n",
        "The ability to model different high dimentional matrices is what makes tensors what they are"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "# when we do this y automatically understands that y is a function of x which itself requires gradients\r\n",
        "y = x + 5\r\n",
        "print(y)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6., 6.],\n",
            "        [6., 6.],\n",
            "        [6., 6.]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "# so we are stackin another fuction on top of y\r\n",
        "z = y*y + 1\r\n",
        "print(z)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[37., 37.],\n",
            "        [37., 37.],\n",
            "        [37., 37.]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "# torch.sum simply adds all the numbers in the tensors\r\n",
        "t = torch.sum(z)\r\n",
        "print(t)\r\n",
        "# We can think of this as a forward pass we have been doing\r\n",
        "# The book-keeping is being kept bu pyTorch and it tells that the last operation being done on the tensor was sum operation"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(222., grad_fn=<SumBackward0>)\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "# At this point we are ready to do a backward pass\r\n",
        "t.backward()\r\n",
        "# nothing is shown in the output, pyTorch is internally doing some computations "
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "# x.grad is the derivative of t wrt x\r\n",
        "# We had taken the backward starting from t so that becomes the fucntion that we want to diffrenciate\r\n",
        "# ans we want do diffrenciate it against x upon which we are calling the grad\r\n",
        "print(x.grad)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[12., 12.],\n",
            "        [12., 12.],\n",
            "        [12., 12.]])\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logic for why the derivative of t wrt x was 12:  \r\n",
        "$t = \\sum_i z_i,    z_i = y_i^2 + 1,    y_i = x_i + 5$\r\n",
        "\r\n",
        "$\\frac{\\partial t}{\\partial x_i} = \\frac{\\partial z_i}{\\partial x_i} = \\frac{\\partial z_i}{\\partial y_i} \\frac{\\partial y_i}{\\partial x_i} = 2y_i \\times 1$\r\n",
        "\r\n",
        "\r\n",
        "At x = 1, y = 6, $\\frac{\\partial t}{\\partial x_i} = 12$"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "So here we can see that we are getting the partial derivative of t wrt all x's.  \r\n",
        "We can now write any cascading set on functions on a given set of inputs,  \r\n",
        "we can call trig functions, tanh, log, even things like standard deviation mean and so on, \r\n",
        "then compute derivative wrt inputs\r\n",
        "\r\n",
        "This is numerically being computed at the poin we have initialised our values. So in this particular example, x is initialised at 1's."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "# Another example, x and y values replacing z by r where r is taking the sigmoid of y\r\n",
        "x = torch.ones([3, 2], requires_grad=True)\r\n",
        "y = x + 5\r\n",
        "r = 1/(1 + torch.exp(-y))\r\n",
        "print(r)\r\n",
        "s = torch.sum(r)\r\n",
        "s.backward()\r\n",
        "print(x.grad)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9975, 0.9975],\n",
            "        [0.9975, 0.9975],\n",
            "        [0.9975, 0.9975]], grad_fn=<MulBackward0>)\n",
            "tensor([[0.0025, 0.0025],\n",
            "        [0.0025, 0.0025],\n",
            "        [0.0025, 0.0025]])\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We were earlier writing the forward pass and backward pass ourselves and were implementing our knowlege of the derivative of sigmoids, tanh etc. now we are letting pyTorch do it for us automatically. So it's quite a powerful thing in that sense!"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "# We can do the above diffrensiation in this manne too, just that here instad of taking r into s and summing it to get one value\r\n",
        "# we define 'a' with 1's and the same shape of x.\r\n",
        "# So basicallly we are avoiding calling the sum fuction\r\n",
        "x = torch.ones([3, 2], requires_grad=True)\r\n",
        "y = x + 5\r\n",
        "r = 1/(1 + torch.exp(-y))\r\n",
        "a = torch.ones([3, 2])\r\n",
        "r.backward(a)\r\n",
        "print(x.grad)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0025, 0.0025],\n",
            "        [0.0025, 0.0025],\n",
            "        [0.0025, 0.0025]])\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "r.backward is computing the derivative of r wrt x, but it multiplies point wise with derivative the value of 'a' which we  have taken as an argument in r.backward   \r\n",
        "so we are doing $\\frac{\\partial{s}}{\\partial{r}}$ and multiplying poin wise with 'a'.  \r\n",
        "\r\n",
        "This feature is there so that we are able to cascade our chain rule through multiple fucntions  "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\frac{\\partial{s}}{\\partial{x}} = \\frac{\\partial{s}}{\\partial{r}} \\cdot \\frac{\\partial{r}}{\\partial{x}}$\r\n",
        "\r\n",
        "For the above code $a$ represents $\\frac{\\partial{s}}{\\partial{r}}$ and then $x.grad$ gives directly $\\frac{\\partial{s}}{\\partial{x}}$\r\n",
        "\r\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "id we want to calculate $\\frac{\\partial{s}}{\\partial{x}}$ then it is given by chain rule $\\frac{\\partial{s}}{\\partial{r}} \\cdot \\frac{\\partial{r}}{\\partial{x}}$  \r\n",
        "So we want to move from s to, so we first move from s to r and then form r to x.  \r\n",
        "r to x is given by r.backward, but if we have already computed s to r and stored that in a then point wise multiplying a with $\\frac{\\partial{r}}{\\partial{x}}$ will directly give us $\\frac{\\partial{s}}{\\partial{x}}$  \r\n",
        "In this case we dont have a s we want to concider so, When a is a submission we will use torch.ones"
      ],
      "metadata": {}
    }
  ]
}