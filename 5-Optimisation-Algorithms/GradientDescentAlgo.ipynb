{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Outline\r\n",
    "\r\n",
    "1. Modified SN class\r\n",
    "2. Overall setup - What is the data, model, task\r\n",
    "3. Plotting functions - 3d, contour\r\n",
    "4. Individual algorithms and how they perform\r\n",
    "5. Exercise"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#used for 3D Plotting\r\n",
    "from mpl_toolkits.mplot3d import Axes3D\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "from matplotlib import cm\r\n",
    "import matplotlib.colors\r\n",
    "\r\n",
    "# For animated plots\r\n",
    "from matplotlib import animation, rc\r\n",
    "from IPython.display import HTML\r\n",
    "\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class SN:\r\n",
    "  #This is a single sigmoid neuron - takes only a single input\r\n",
    "  '''\r\n",
    "  We are taking the w_init & b_ init which are the initial value we will like to have for w and b\r\n",
    "  algo argument - tells which algo do we want to use for Gradient Descent\r\n",
    "  We have a few array/list varibes, suffixed with '_h' this indicated that these are history variables.\r\n",
    "  w_h,b_h are self explanatory, e_h tracks the error/loss at the current moment\r\n",
    "  algo is being stored in an internal variable.\r\n",
    "  '''\r\n",
    "  def __init__(self, w_init, b_init, algo):\r\n",
    "    self.w = w_init\r\n",
    "    self.b = b_init\r\n",
    "    self.w_h = []\r\n",
    "    self.b_h = []\r\n",
    "    self.e_h = []\r\n",
    "    self.algo = algo\r\n",
    "  \r\n",
    "  '''  \r\n",
    "  Now the signoid function additionaly takes w, b as inputs. \r\n",
    "  We will be calculating the sigmoid value currently moduled by the class,\r\n",
    "  but there is a need to run the sigmoid function for some other values\r\n",
    "  so here if w and b if spefied we will use them or else we will use the default\r\n",
    "  '''\r\n",
    "  def sigmoid(self, x, w=None, b=None):\r\n",
    "    if w is None:\r\n",
    "      w = self.w\r\n",
    "    if b is None:\r\n",
    "      b = self.b\r\n",
    "    return 1. / (1. + np.exp(-(w*x + b))) # here w and b are scalar inputs\r\n",
    "  \r\n",
    "  '''  \r\n",
    "  Given an X and Y which is a training data that we have and also \r\n",
    "  w,b optionally spefied like the above sigmoid function \r\n",
    "  we would line to compute the MSE- mean squared error    \r\n",
    "  '''\r\n",
    "  def error(self, X, Y, w=None, b=None):\r\n",
    "    if w is None:\r\n",
    "      w = self.w\r\n",
    "    if b is None:\r\n",
    "      b = self.b\r\n",
    "    err = 0\r\n",
    "    for x, y in zip(X, Y):\r\n",
    "      err += 0.5 * (self.sigmoid(x, w, b) - y) ** 2\r\n",
    "    return err\r\n",
    "  '''\r\n",
    "  returns the gradient of the sigoid w.r.t to the input\r\n",
    "  We have the ability to use some other values of w and b \r\n",
    "  just as in the case of sigmoid function \r\n",
    "  to calculate the sigmoid at some other fx^n not that we are currently at\r\n",
    "  '''\r\n",
    "  def grad_w(self, x, y, w=None, b=None):\r\n",
    "    if w is None:\r\n",
    "      w = self.w\r\n",
    "    if b is None:\r\n",
    "      b = self.b\r\n",
    "    y_pred = self.sigmoid(x, w, b)\r\n",
    "    return (y_pred - y) * y_pred * (1 - y_pred) * x\r\n",
    "\r\n",
    "  '''  \r\n",
    "  same applies for grad b as in grad a\r\n",
    "  The only change is that the * x in return goes away\r\n",
    "  therefore it produces the gradienof the loss (in the error fx^n) w.r.t w & b\r\n",
    "  '''\r\n",
    "  def grad_b(self, x, y, w=None, b=None):\r\n",
    "    if w is None:\r\n",
    "      w = self.w\r\n",
    "    if b is None:\r\n",
    "      b = self.b\r\n",
    "    y_pred = self.sigmoid(x, w, b)\r\n",
    "    return (y_pred - y) * y_pred * (1 - y_pred)\r\n",
    "  \r\n",
    "  '''  \r\n",
    "  The fit fx^n takes X and Y the training data and a bunch of other stuff,\r\n",
    "  epochs - no. of times we go over the data, eta - learning rate\r\n",
    "  other parameters are comming fromt the different GD learning algo\r\n",
    "  thery have been sassigned default value which ofcourse can vbe overwritten\r\n",
    "  '''\r\n",
    "  def fit(self, X, Y, \r\n",
    "          epochs=100, eta=0.01, gamma=0.9, mini_batch_size=100, eps=1e-8,  \r\n",
    "          beta=0.9, beta1=0.9, beta2=0.9\r\n",
    "         ):\r\n",
    "    self.w_h = []\r\n",
    "    self.b_h = []\r\n",
    "    self.e_h = []\r\n",
    "    self.X = X\r\n",
    "    self.Y = Y\r\n",
    "    \r\n",
    "    # There are many of algos for the different types of Gradient Descent Learning Algos:\r\n",
    "    # These algos are avilable in the init fx^n's itself as we saw above\r\n",
    "    \r\n",
    "    '''\r\n",
    "    This is the Gradient decent that we are already familiar with\r\n",
    "    we simply iterate over all the data eopchs no. of times\r\n",
    "    each case we initialise dw and db value\r\n",
    "    for every inpu we accumalate dw and db values, by calling the respective grad fucntions\r\n",
    "    then we have the update rule, this take only one parameter for the fit function\r\n",
    "    which is the learning rate - eta\r\n",
    "    after ebery update to w and b we call the append log functions\r\n",
    "    It apppends the current value of weight and bias into the list that we are maintining\r\n",
    "    and also calculates the error at the current point\r\n",
    "    '''\r\n",
    "    if self.algo == 'GD':\r\n",
    "      for i in range(epochs):\r\n",
    "        dw, db = 0, 0\r\n",
    "        for x, y in zip(X, Y):\r\n",
    "          dw += self.grad_w(x, y)\r\n",
    "          db += self.grad_b(x, y)\r\n",
    "        self.w -= eta * dw / X.shape[0]\r\n",
    "        self.b -= eta * db / X.shape[0]\r\n",
    "        self.append_log()\r\n",
    "        \r\n",
    "    elif self.algo == 'MiniBatch':\r\n",
    "      for i in range(epochs):\r\n",
    "        dw, db = 0, 0\r\n",
    "        points_seen = 0\r\n",
    "        for x, y in zip(X, Y):\r\n",
    "          dw += self.grad_w(x, y)\r\n",
    "          db += self.grad_b(x, y)\r\n",
    "          points_seen += 1\r\n",
    "          if points_seen % mini_batch_size == 0:\r\n",
    "            self.w -= eta * dw / mini_batch_size\r\n",
    "            self.b -= eta * db / mini_batch_size\r\n",
    "            self.append_log()\r\n",
    "            dw, db = 0, 0\r\n",
    "        \r\n",
    "    elif self.algo == 'Momentum':\r\n",
    "      v_w, v_b = 0, 0\r\n",
    "      for i in range(epochs):\r\n",
    "        dw, db = 0, 0\r\n",
    "        for x, y in zip(X, Y):\r\n",
    "          dw += self.grad_w(x, y)\r\n",
    "          db += self.grad_b(x, y)\r\n",
    "        v_w = gamma * v_w + eta * dw \r\n",
    "        v_b = gamma * v_b + eta * db\r\n",
    "        self.w = self.w - v_w\r\n",
    "        self.b = self.b - v_b\r\n",
    "        self.append_log()\r\n",
    "        \r\n",
    "    elif self.algo == 'NAG':\r\n",
    "      v_w, v_b = 0, 0\r\n",
    "      for i in range(epochs):\r\n",
    "        dw, db = 0, 0\r\n",
    "        v_w = gamma * v_w\r\n",
    "        v_b = gamma * v_b\r\n",
    "        for x, y in zip(X, Y):\r\n",
    "          dw += self.grad_w(x, y, self.w - v_w, self.b - v_b)\r\n",
    "          db += self.grad_b(x, y, self.w - v_w, self.b - v_b)\r\n",
    "        v_w = v_w + eta * dw\r\n",
    "        v_b = v_b + eta * db\r\n",
    "        self.w = self.w - v_w\r\n",
    "        self.b = self.b - v_b\r\n",
    "        self.append_log()\r\n",
    "  \r\n",
    "  '''It apppends the current value of weight and bias into the list that we are maintining\r\n",
    "    and also calculates the error at the current point'''\r\n",
    "  def append_log(self):\r\n",
    "    self.w_h.append(self.w)\r\n",
    "    self.b_h.append(self.b)\r\n",
    "    self.e_h.append(self.error(self.X, self.Y))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Config parameters for an experiment - will change according to the experiment we are running\r\n",
    "X = np.asarray([3.5, 0.35, 3.2, -2.0, 1.5, -0.5])\r\n",
    "Y = np.asarray([0.5, 0.50, 0.5,  0.5, 0.1,  0.3])\r\n",
    "\r\n",
    "algo = 'GD'\r\n",
    "\r\n",
    "w_init = 2\r\n",
    "b_init = -2\r\n",
    "\r\n",
    "w_min = -7\r\n",
    "w_max = 5\r\n",
    "\r\n",
    "b_min = -5\r\n",
    "b_max = 5\r\n",
    "\r\n",
    "epochs = 1000\r\n",
    "# mini_batch_size = 6\r\n",
    "# gamma = 0.9\r\n",
    "eta = 1\r\n",
    "\r\n",
    "animation_frames = 20\r\n",
    "\r\n",
    "plot_2d = True\r\n",
    "plot_3d = False # contour plots"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_animate_3d(i):\r\n",
    "  i = int(i*(epochs/animation_frames))\r\n",
    "  line1.set_data(sn.w_h[:i+1], sn.b_h[:i+1])\r\n",
    "  line1.set_3d_properties(sn.e_h[:i+1])\r\n",
    "  line2.set_data(sn.w_h[:i+1], sn.b_h[:i+1])\r\n",
    "  line2.set_3d_properties(np.zeros(i+1) - 1)\r\n",
    "  title.set_text('Epoch: {: d}, Error: {:.4f}'.format(i, sn.e_h[i]))\r\n",
    "  return line1, line2, title"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Cells specifies what to do plot_3d is called\r\n",
    "if plot_3d: \r\n",
    "  W = np.linspace(w_min, w_max, 256) # linearspace range and then how may points we will like to have(256)\r\n",
    "  b = np.linspace(b_min, b_max, 256)\r\n",
    "  \r\n",
    "  # We will make a meshgrid -  Which takes all possible combinations of w and b\r\n",
    "  WW, BB = np.meshgrid(W, b)\r\n",
    "  \r\n",
    "  # For each pair of w and b using meshgrid we will call the sn.error function\r\n",
    "  # X and Y values are Training data, but we are giveing individual value of W and b from our sample list.\r\n",
    "  # So basically w and b are specified externally here as we have seen we had a option to do that in the sigomid class\r\n",
    "  Z = sn.error(X, Y, WW, BB)\r\n",
    "\r\n",
    "  fig = plt.figure(dpi=100) # If we increase dip the size of the image is increased and so is the quality\r\n",
    "  ax = fig.gca(projection='3d') # ax is a handle which is then used to call its functionality elsewhere as we can see below\r\n",
    "  \r\n",
    "  # We will do 2 kinds of plot a 3D pot which will actually show the 3D plot of the error wrt the weight and the bias \r\n",
    "  surf = ax.plot_surface(WW, BB, Z, rstride=3, cstride=3, alpha=0.5, cmap=cm.coolwarm, linewidth=0, antialiased=False)\r\n",
    "  # The secnond kind of plot is a conotur plot. We are plotting 25 different countour lines. direction of countouring is on the z axis\r\n",
    "  cset = ax.contourf(WW, BB, Z, 25, zdir='z', offset=-1, alpha=0.6, cmap=cm.coolwarm)\r\n",
    "  \r\n",
    "  # Settingt the limits we have used for searching these points and adding them by one to have enough room\r\n",
    "  ax.set_xlabel('w')\r\n",
    "  ax.set_xlim(w_min - 1, w_max + 1)\r\n",
    "  ax.set_ylabel('b')\r\n",
    "  ax.set_ylim(b_min - 1, b_max + 1)\r\n",
    "  ax.set_zlabel('error')\r\n",
    "  ax.set_zlim(-1, np.max(Z))\r\n",
    "  \r\n",
    "  # When  plotting a 3D plot it is important to see where we would like to see the plot from(form which direction)\r\n",
    "  ax.view_init (elev=25, azim=-75) # azim = -20\r\n",
    "  ax.dist=12  \r\n",
    "  title = ax.set_title('Epoch 0')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# To show what the algo is doing dynamically, which is captured in our history variable \r\n",
    "if plot_3d: \r\n",
    "  i = 0\r\n",
    "  # w_h, b_h and e_h, corresponding to the wiighs, bias and error respectively\r\n",
    "  # Aling with the surface plot we are showing a scatter plot \r\n",
    "  line1, = ax.plot(sn.w_h[:i+1], sn.b_h[:i+1], sn.e_h[:i+1], color='black',marker='.')\r\n",
    "  line2, = ax.plot(sn.w_h[:i+1], sn.b_h[:i+1], np.zeros(i+1) - 1, color='red', marker='.')\r\n",
    "  \r\n",
    "  # As we want to create an animation we are callin the FuncAnimation it is taking the figure handle\r\n",
    "  # We are caliing the plot animate function, described above everytime we want to generate a new frame\r\n",
    "  anim = animation.FuncAnimation(fig, func=plot_animate_3d, frames=animation_frames)\r\n",
    "  rc('animation', html='jshtml')\r\n",
    "  anim"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "96eb50e1d44aed467dc8f759cb08c32fbfa9babcf79c554e2d0e5feb04653a10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}