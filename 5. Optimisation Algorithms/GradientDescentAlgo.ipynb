{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Outline\r\n",
    "\r\n",
    "1. Modified SN class\r\n",
    "2. Overall setup - What is the data, model, task\r\n",
    "3. Plotting functions - 3d, contour\r\n",
    "4. Individual algorithms and how they perform\r\n",
    "5. Exercise"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#used for 3D Plotting\r\n",
    "from mpl_toolkits.mplot3d import Axes3D\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "from matplotlib import cm\r\n",
    "import matplotlib.colors\r\n",
    "\r\n",
    "# For animated plots\r\n",
    "from matplotlib import animation, rc\r\n",
    "from IPython.display import HTML\r\n",
    "\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class SN:\r\n",
    "  #This is a single sigmoid neuron - takes only a single input\r\n",
    "  '''\r\n",
    "  We are taking the w_init & b_ init which are the initial value we will like to have for w and b\r\n",
    "  algo argument - tells which algo do we want to use for Gradient Descent\r\n",
    "  We have a few array/list varibes, suffixed with '_h' this indicated that these are history variables.\r\n",
    "  w_h,b_h are self explanatory, e_h tracks the error/loss at the current moment\r\n",
    "  algo is being stored in an internal variable.\r\n",
    "  '''\r\n",
    "  def __init__(self, w_init, b_init, algo):\r\n",
    "    self.w = w_init\r\n",
    "    self.b = b_init\r\n",
    "    self.w_h = []\r\n",
    "    self.b_h = []\r\n",
    "    self.e_h = []\r\n",
    "    self.algo = algo\r\n",
    "  \r\n",
    "  '''  \r\n",
    "  Now the signoid function additionaly takes w, b as inputs. \r\n",
    "  We will be calculating the sigmoid value currently moduled by the class,\r\n",
    "  but there is a need to run the sigmoid function for some other values\r\n",
    "  so here if w and b if spefied we will use them or else we will use the default\r\n",
    "  '''\r\n",
    "  def sigmoid(self, x, w=None, b=None):\r\n",
    "    if w is None:\r\n",
    "      w = self.w\r\n",
    "    if b is None:\r\n",
    "      b = self.b\r\n",
    "    return 1. / (1. + np.exp(-(w*x + b))) # here w and b are scalar inputs\r\n",
    "  \r\n",
    "  '''  \r\n",
    "  Given an X and Y which is a training data that we have and also \r\n",
    "  w,b optionally spefied like the above sigmoid function \r\n",
    "  we would line to compute the MSE- mean squared error    \r\n",
    "  '''\r\n",
    "  def error(self, X, Y, w=None, b=None):\r\n",
    "    if w is None:\r\n",
    "      w = self.w\r\n",
    "    if b is None:\r\n",
    "      b = self.b\r\n",
    "    err = 0\r\n",
    "    for x, y in zip(X, Y):\r\n",
    "      err += 0.5 * (self.sigmoid(x, w, b) - y) ** 2\r\n",
    "    return err\r\n",
    "  '''\r\n",
    "  returns the gradient of the sigoid w.r.t to the input\r\n",
    "  We have the ability to use some other values of w and b \r\n",
    "  just as in the case of sigmoid function \r\n",
    "  to calculate the sigmoid at some other fx^n not that we are currently at\r\n",
    "  '''\r\n",
    "  def grad_w(self, x, y, w=None, b=None):\r\n",
    "    if w is None:\r\n",
    "      w = self.w\r\n",
    "    if b is None:\r\n",
    "      b = self.b\r\n",
    "    y_pred = self.sigmoid(x, w, b)\r\n",
    "    return (y_pred - y) * y_pred * (1 - y_pred) * x\r\n",
    "\r\n",
    "  '''  \r\n",
    "  same applies for grad b as in grad a\r\n",
    "  The only change is that the * x in return goes away\r\n",
    "  therefore it produces the gradienof the loss (in the error fx^n) w.r.t w & b\r\n",
    "  '''\r\n",
    "  def grad_b(self, x, y, w=None, b=None):\r\n",
    "    if w is None:\r\n",
    "      w = self.w\r\n",
    "    if b is None:\r\n",
    "      b = self.b\r\n",
    "    y_pred = self.sigmoid(x, w, b)\r\n",
    "    return (y_pred - y) * y_pred * (1 - y_pred)\r\n",
    "  \r\n",
    "  '''  \r\n",
    "  The fit fx^n takes X and Y the training data and a bunch of other stuff,\r\n",
    "  epochs - no. of times we go over the data, eta - learning rate\r\n",
    "  other parameters are comming fromt the different GD learning algo\r\n",
    "  thery have been sassigned default value which ofcourse can vbe overwritten\r\n",
    "  '''\r\n",
    "  def fit(self, X, Y, \r\n",
    "          epochs=100, eta=0.01, gamma=0.9, mini_batch_size=100, eps=1e-8,  \r\n",
    "          beta=0.9, beta1=0.9, beta2=0.9\r\n",
    "         ):\r\n",
    "    self.w_h = []\r\n",
    "    self.b_h = []\r\n",
    "    self.e_h = []\r\n",
    "    self.X = X\r\n",
    "    self.Y = Y\r\n",
    "    \r\n",
    "    # There are many of algos for the different types of Gradient Descent Learning Algos:\r\n",
    "    # These algos are avilable in the init fx^n's itself as we saw above\r\n",
    "    \r\n",
    "    '''\r\n",
    "    This is the Gradient decent that we are already familiar with\r\n",
    "    we simply iterate over all the data eopchs no. of times\r\n",
    "    each case we initialise dw and db value\r\n",
    "    for every inpu we accumalate dw and db values, by calling the respective grad fucntions\r\n",
    "    then we have the update rule, this take only one parameter for the fit function\r\n",
    "    which is the learning rate - eta\r\n",
    "    after ebery update to w and b we call the append log functions\r\n",
    "    It apppends the current value of weight and bias into the list that we are maintining\r\n",
    "    and also calculates the error at the current point\r\n",
    "    '''\r\n",
    "    if self.algo == 'GD':\r\n",
    "      for i in range(epochs):\r\n",
    "        dw, db = 0, 0\r\n",
    "        for x, y in zip(X, Y):\r\n",
    "          dw += self.grad_w(x, y)\r\n",
    "          db += self.grad_b(x, y)\r\n",
    "        self.w -= eta * dw / X.shape[0]\r\n",
    "        self.b -= eta * db / X.shape[0]\r\n",
    "        self.append_log()\r\n",
    "        \r\n",
    "    elif self.algo == 'MiniBatch':\r\n",
    "      for i in range(epochs):\r\n",
    "        dw, db = 0, 0\r\n",
    "        points_seen = 0\r\n",
    "        for x, y in zip(X, Y):\r\n",
    "          dw += self.grad_w(x, y)\r\n",
    "          db += self.grad_b(x, y)\r\n",
    "          points_seen += 1\r\n",
    "          if points_seen % mini_batch_size == 0:\r\n",
    "            self.w -= eta * dw / mini_batch_size\r\n",
    "            self.b -= eta * db / mini_batch_size\r\n",
    "            self.append_log()\r\n",
    "            dw, db = 0, 0\r\n",
    "        \r\n",
    "    elif self.algo == 'Momentum':\r\n",
    "      v_w, v_b = 0, 0\r\n",
    "      for i in range(epochs):\r\n",
    "        dw, db = 0, 0\r\n",
    "        for x, y in zip(X, Y):\r\n",
    "          dw += self.grad_w(x, y)\r\n",
    "          db += self.grad_b(x, y)\r\n",
    "        v_w = gamma * v_w + eta * dw \r\n",
    "        v_b = gamma * v_b + eta * db\r\n",
    "        self.w = self.w - v_w\r\n",
    "        self.b = self.b - v_b\r\n",
    "        self.append_log()\r\n",
    "        \r\n",
    "    elif self.algo == 'NAG':\r\n",
    "      v_w, v_b = 0, 0\r\n",
    "      for i in range(epochs):\r\n",
    "        dw, db = 0, 0\r\n",
    "        v_w = gamma * v_w\r\n",
    "        v_b = gamma * v_b\r\n",
    "        for x, y in zip(X, Y):\r\n",
    "          dw += self.grad_w(x, y, self.w - v_w, self.b - v_b)\r\n",
    "          db += self.grad_b(x, y, self.w - v_w, self.b - v_b)\r\n",
    "        v_w = v_w + eta * dw\r\n",
    "        v_b = v_b + eta * db\r\n",
    "        self.w = self.w - v_w\r\n",
    "        self.b = self.b - v_b\r\n",
    "        self.append_log()\r\n",
    "  \r\n",
    "  '''It apppends the current value of weight and bias into the list that we are maintining\r\n",
    "    and also calculates the error at the current point'''\r\n",
    "  def append_log(self):\r\n",
    "    self.w_h.append(self.w)\r\n",
    "    self.b_h.append(self.b)\r\n",
    "    self.e_h.append(self.error(self.X, self.Y))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}